\documentclass[
  doc,
  longtable,
  nolmodern,
  notxfonts,
  notimes,
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{apa7}

\usepackage{amsmath}
\usepackage{amssymb}

\geometry{inner=1in, outer=1in}
\fancyhfoffset[LE,RO]{0cm}



\RequirePackage{longtable}
\RequirePackage{threeparttablex}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-.5em}%
	{\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{0.5em}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-\z@\relax}%
	{\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother




\usepackage{longtable, booktabs, multirow, multicol, colortbl, hhline, caption, array, float, xpatch}
\usepackage{subcaption}


\renewcommand\thesubfigure{\Alph{subfigure}}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{tcolorbox}
\tcbuselibrary{listings,theorems, breakable, skins}
\usepackage{fontawesome5}

\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{ACACAC}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582EC}
\definecolor{quarto-callout-important-color-frame}{HTML}{D9534F}
\definecolor{quarto-callout-warning-color-frame}{HTML}{F0AD4E}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02B875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{FD7E14}

%\newlength\Oldarrayrulewidth
%\newlength\Oldtabcolsep


\usepackage{hyperref}




\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}





\usepackage{newtx}

\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}





\title{Tracking structural cues or relying on probabilistic inference in
Turkish agreement?}


\shorttitle{Structural vs.~Probabilistic Inference}


\usepackage{etoolbox}








\authorsnames{Ozge Bakay,Utku Turk,Duygu Demiray,Brian Dillon}





\affiliation{
{}}




\leftheader{Bakay, Turk, Demiray and Dillon}



\abstract{Agreement attraction is a type of illusion where a plural
attractor noun makes an ungrammatical sentence appear acceptable, as in
'*The key to the cabinets are rusty.' This study investigates the
mechanisms driving such errors in Turkish, focusing on the processing of
nominal vs.~genitive attractors. Prior research contrasts
retrieval-based accounts, which implicate feature mismatches or
structural confusion, with probabilistic inference accounts, which view
attraction as a rational repair strategy. Using a speeded acceptability
task (N=59), we analyzed the impact of attractor case (Nominative
vs.~Genitive) and number match on agreement processing. Corpus analyses
indicated higher transitional probabilities for plural targets following
Genitive attractors. However, our Bayesian analysis revealed comparable
attraction effects for both case markings, supporting structural
controllerhood theories over surface cue-based or pure probabilistic
inference accounts. These findings suggest that the structural potential
to control agreement outweighs surface feature overlap or transitional
probabilities in driving attraction effects. }

\keywords{Turkish, psycholinguistics, agreement attraction, memory
retrieval, Bayesian analysis}

\authornote{ 
\par{ }
\par{       Author roles were classified using the Contributor Role
Taxonomy (CRediT; https://credit.niso.org/) as follows:  Ozge
Bakay:   Conceptualization, Methodology, Descriptive and Statistical
Analysis, Writing - Original Draft; Utku
Turk:   Conceptualization, Methodology, Descriptive and Statistical
Analysis, Writing - Original Draft, Large Language Model Analysis; Duygu
Demiray:   Large Language Model Analysis, Writing - Original
Draft; Brian Dillon:   Funding Acquisition, Conceptualization, Writing -
Review \& Editing}
\par{Correspondence concerning this article should be addressed to Ozge
Bakay, Email: \href{mailto:obakay@umass.edu}{obakay@umass.edu}}
}

\makeatletter
\let\endoldlt\endlongtable
\def\endlongtable{
\hline
\endoldlt
}
\makeatother

\urlstyle{same}



\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% From https://tex.stackexchange.com/a/645996/211326
%%% apa7 doesn't want to add appendix section titles in the toc
%%% let's make it do it
\makeatletter
\xpatchcmd{\appendix}
  {\par}
  {\addcontentsline{toc}{section}{\@currentlabelname}\par}
  {}{}
\makeatother

%% Disable longtable counter
%% https://tex.stackexchange.com/a/248395/211326

\usepackage{etoolbox}

\makeatletter
\patchcmd{\LT@caption}
  {\bgroup}
  {\bgroup\global\LTpatch@captiontrue}
  {}{}
\patchcmd{\longtable}
  {\par}
  {\par\global\LTpatch@captionfalse}
  {}{}
\apptocmd{\endlongtable}
  {\ifLTpatch@caption\else\addtocounter{table}{-1}\fi}
  {}{}
\newif\ifLTpatch@caption
\makeatother

\begin{document}

\maketitle




\setlength\LTleft{0pt}




\section{Introduction}\label{introduction}

Agreement attraction is a pervasive phenomenon in sentence processing
where a verb agrees with a noun phrase other than its grammatical
subject. For instance, in the sentence ``\emph{The key to the cabinets
are rusty},'' the plural attractor ``cabinets'' creates an illusion of
acceptability despite the singular subject ``key.'' This phenomenon has
been extensively studied to understand the mechanisms of memory
retrieval and structural dependency formation during sentence
comprehension.

\subsection{Models of Agreement
Attraction}\label{models-of-agreement-attraction}

Two primary classes of theories have been proposed to explain agreement
attraction: retrieval-based accounts and probabilistic inference
accounts.

\subsubsection{Retrieval-Based Accounts}\label{retrieval-based-accounts}

Retrieval-based accounts posit that attraction arises from the
architectural constraints of the working memory system. Following the
cue-based retrieval framework Vasishth and Lewis
(\citeproc{ref-vasishth2008integration}{2008}); Wagers et al.
(\citeproc{ref-wagers2009agreement}{2009}), dependency formation relies
on a content-addressable memory system. When the parser encounters a
verb, it initiates a search using a set of retrieval cues. The nature of
these cues, however, remains a point of debate, leading to two distinct
sets of predictions for the current study.

\textbf{1. Surface Feature Cues}: Standard implementations assume that
retrieval cues reflect the morphosyntactic features of the expected
subject, such as \texttt{\{+Plural,\ +Nominative\}}. Under this view,
attraction is driven by the feature overlap between the attractor and
the retrieval cues. Since Turkish subjects are canonically Nominative
(bare), a Nominative attractor provides a better match to the
\texttt{+Nominative} cue than a Genitive attractor. Consequently, a
surface feature account predicts \textbf{stronger attraction for
Nominative (Bare) attractors} than for Genitive attractors.

\textbf{2. Structural Controllerhood}: Alternatively, the retrieval
system may prioritize items that hold a structural status associated
with agreement control. Bhatia and Dillon
(\citeproc{ref-bhatia2011agreement}{2011}) proposed the ``structural
controllerhood'' hypothesis, suggesting that nouns in positions that
typically license agreement are more accessible or ``active'' in memory.
In Turkish, both Nominative subjects and Genitive possessors serve as
agreement controllers. If the parser targets ``controller-like''
structural representations, both attractor types should be strong
candidates for retrieval. Thus, the structural controllerhood hypothesis
predicts \textbf{comparable attraction effects} for both Genitive and
Nominative attractors, despite their case differences.

\subsubsection{Probabilistic Inference and Rational
Adaptation}\label{probabilistic-inference-and-rational-adaptation}

Alternatively, probabilistic inference accounts, often framed within the
noisy-channel framework, view language comprehension as a process of
rational inference under uncertainty. From this perspective, the parser
assumes that the linguistic input is liable to noise (e.g., production
errors, perceptual/transmission noise). The goal of the comprehender is
to reconstruct the intended message (\(S\)) given the noisy perceptual
input (\(I\)) by maximizing the posterior probability
\(P(S|I) \propto P(I|S) P(S)\).

In the context of agreement attraction, the listener might infer that
the speaker intended to say a grammatical alternative (e.g., ``\emph{The
keys to the cabinets are rusty}'') but produced an error, or that they
misheard the input. Crucially, this inference is sensitive to prior
probabilities (\(P(S)\)). If the context makes a plural subject highly
probable, or if the transition from the attractor to a plural verb is
highly frequent, the parser is more likely to ``hallucinate'' a
grammatical structure, leading to higher acceptance rates Linzen et al.
(\citeproc{ref-linzen2016assessing}{2016}). Thus, under this view,
attraction is not a failure of memory, but an optimal adaptation to a
noisy environment.

Recent work by Cartner et al. (\citeproc{ref-cartner2025number}{2025})
has specifically explored these probabilistic dynamics in Turkish. They
argue that the strong predictive validity of Genitive case for upcoming
agreement targets should make the parser more sensitive to number
mismatches in Genitive constructions. Specifically, if the system is
tracking the conditional probability
\(P(\text{Plural Target} | \text{Attractor})\), the high predictability
of the Genitive attractor should lead to stronger ``repair'' effects
when a plural marker is encountered on the verb, predicting a larger
attraction effect compared to the less predictive Nominative attractors.
This sets up a clear contrast with structural accounts, which predict no
such modulation by case-based probabilities.

\subsection{The Present Study: Turkish
Possessives}\label{the-present-study-turkish-possessives}

Turkish offers a unique testing ground for distinguishing these
accounts. In Turkish possessive constructions, attractors can be marked
with either Genitive (GEN) case or left bare (Nominative/NOM). -
\textbf{Genitive Attractors}: In phrases like \emph{Mülteci-nin
avukat-ı} (`The refugee's lawyer'), the genitive noun is a structural
possessor. In Turkish, possessors control agreement on the head noun,
making them highly ``controller-like'' structurally. -
\textbf{Nominative Attractors}: Nominative nouns are the canonical
subjects and agreement controllers in the language.

While both are associated with the abstract structural role of being an
agreement controller, they differ significantly in their distributional
properties. Our corpus analysis (see Results) shows that GEN-marked
attractors strongly predict a following plural target (agreement
bearer), whereas bare attractors do not.

If attraction is driven by \textbf{probabilistic inference} (tracking
surface statistics and transitional probabilities), we should see
significantly stronger attraction effects with Genitive attractors,
which strongly predict plural agreement targets. Conversely, if
attraction is driven by \textbf{structural controllerhood} (retrieving
items that structurally resemble controllers), both Genitive and
Nominative attractors should elicit comparable attraction effects, as
both represent high-salience controller roles in the Turkish grammar.

In this study, we directly compare these predictions using a speeded
acceptability judgment task. We manipulate the case of the attractor
(GEN vs.~Bare) to determine whether the agreement mechanism is sensitive
to fine-grained probabilistic expectations or robust structural cues.

\section{Methods}\label{methods}

\subsection{Participants}\label{participants}

Fifty-nine participants were recruited for this study (\texttt{N} = 59).
All participants were native speakers of Turkish and gave informed
consent.

\subsection{Materials}\label{materials}

We constructed 24 sets of experimental items in a 2x3 within-subjects
design. The two factors were \textbf{Attractor Case} (Genitive
vs.~Nominative/Bare) and \textbf{Agreement Match} (Target Match,
Attractor Match, No Match).

In the \textbf{Target Match} condition, the head noun (Target) was
plural, and the verb was plural (Grammatical). In the \textbf{Attractor
Match} condition, the attractor was plural, the target was singular, but
the verb was plural (Ungrammatical, Attraction). In the \textbf{No
Match} condition, both the target and attractor were singular, and the
verb was plural (Ungrammatical, Baseline).

The stimuli were constructed such that the Genitive attractor appeared
in a possessor position, while the Nominative attractor appeared in a
modifier position within the noun phrase.

\emph{Table 1. Example Stimuli}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2639}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Condition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Sentence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gloss
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Target Match (GEN) & \emph{Mülteci-nin avukat-lar-ı bağırdılar.} &
Refugee-GEN lawyer-PL-POSS shouted. \\
Attractor Match (GEN) & \emph{Mülteci-ler-in avukat-ı bağırdılar.} &
Refugee-PL-GEN lawyer-POSS shouted. \\
No Match (GEN) & \emph{Mülteci-nin avukat-ı bağırdılar.} & Refugee-GEN
lawyer-POSS shouted. \\
\end{longtable}

\subsection{Procedure}\label{procedure}

Participants were recruited online and the experiment was hosted on the
\textbf{PCIBex} farm. The experiment followed a \textbf{Latin Square}
design, ensuring that each participant saw only one version of each item
set, distributed across lists. Sentences were presented word-by-word
(RSVP) to ensure timed reading: each word was presented for \textbf{300
ms}, and participants had a maximum of \textbf{3000 ms} to make their
decision. Participants were instructed to judge whether the sentence was
acceptable or not as quickly and accurately as possible.

\includegraphics[width=6.5in,height=0.63in]{paper_files/figure-latex/mermaid-figure-1.png}

Trial Structure (RSVP Pattern)

\subsection{Computational Modeling}\label{computational-modeling}

To complement our behavioral results, we analyzed the internal
representations of Transformer-based Large Language Models (LLMs),
specifically BERT, using methods established by
(\citeproc{ref-voita2019analyzing}{\textbf{voita2019analyzing?}}) and
Ryu and Lewis (\citeproc{ref-ryu2025agreement}{2025}). Recent work in
computational psycholinguistics suggests that attention heads in these
models can specialize in tracking specific syntactic dependencies,
serving as a computational proxy for the ``retrieval cues'' or
structural access mechanisms in human processing.

\includegraphics[width=6.5in,height=0.51in]{paper_files/figure-latex/mermaid-figure-2.png}

Attention Analysis Pipeline

We identified attention heads that reliably track the subject
(\texttt{nsubj}) dependency in Turkish. For each experimental sentence,
we extracted the attention weights from the verb to the \textbf{Target
Subject} (\(w_{target}\)) and the \textbf{Attractor}
(\(w_{attractor}\)). We calculated the \textbf{Attention Difference}:
\[ \text{AttnDiff} = w_{attractor} - w_{target} \] Positive values
indicate that the model attends more to the attractor than the
grammatical subject, signaling potential interference or ``attraction''
in the model's dependency resolution process.

\subsection{Data Analysis}\label{data-analysis}

We analyzed the response accuracy using \textbf{Bayesian generalized
linear mixed-effects models (GLMM)} with a Bernoulli link function,
implemented in the \texttt{brms} package in R. The models included fixed
effects for Case and Agreement Match, as well as their interaction. We
employed a custom contrast coding scheme to isolate the relevant
theoretical comparisons: 1. \textbf{Grammaticality}: Comparing the
Grammatical condition (Target Match) against the mean of the two
Ungrammatical conditions (Attractor Match + No Match). (Weights: 2, -1,
-1) 2. \textbf{Attraction}: Comparing the Attractor Match condition
against the No Match baseline within the ungrammatical sentences.
(Weights: 0, 1, -1) 3. \textbf{Case}: Sum-coded (Genitive = 1,
Nominative = -1).

\textbf{Exclusion Criteria}: Participants who performed with less than
\textbf{80\% accuracy on filler items} were excluded from the analysis
to ensure data quality. We used a \textbf{maximal random effects
structure}, including random intercepts and slopes for all fixed effects
and their interactions by both participants and items, to ensure the
generalizability of our results.

\section{Results}\label{results}

\subsection{Computational Modeling
Results}\label{computational-modeling-results}

Figure 1 presents the attention difference scores from the identified
``Subject heads'' in Turkish BERT. The model shows positive attention
difference (interference) for both Genitive and Nominative attractors
when they are plural. Crucially, the model does not show a strong
preference for the Genitive attractor over the Nominative one; both
receive significant attention, mirroring the ``structural
controllerhood'' finding in humans.

\begin{figure}[H]

\caption{Attention Difference (Attractor - Target) in Turkish BERT.
Positive values indicate higher attention to the attractor.}

{\centering \pandocbounded{\includegraphics[keepaspectratio]{paper_files/figure-pdf/attention_plot-1.pdf}}

}

\end{figure}%

\subsection{Data Processing}\label{data-processing}

We collected data from 59 participants. 0 participants were excluded due
to low accuracy (\textless{} 80\%) on practice items, leaving a final
sample of 59 participants for analysis. THIS NUMBERS ARE FAKE RIGHT NOW.

\subsection{Descriptive Statistics}\label{descriptive-statistics}

Figure 2 shows the mean acceptance rates across conditions.

For Genitive attractors, participants gave more `yes' responses in the
Attractor Match condition (\emph{M} = 34.3\%) compared to the No Match
condition (\emph{M} = 29.2\%). Similarly, for Nominative attractors,
acceptance rates were higher in the Attractor Match condition (\emph{M}
= 28.4\%) than in the No Match condition (\emph{M} = 18.6\%). This
indicates a clear attraction effect across both case markings.

\begin{figure}[H]

\caption{Mean percentages of `Yes' responses. Error bars represent
standard errors.}

{\centering \pandocbounded{\includegraphics[keepaspectratio]{paper_files/figure-pdf/descriptive_plot-1.pdf}}

}

\end{figure}%

\subsection{Corpus Analysis}\label{corpus-analysis}

To assess the probabilistic predictions, we analyzed the TrTenTen corpus
(\textasciitilde4.9 billion words). We calculated the conditional
probability of a plural target given a plural attractor for both case
markings. The results (Table 2) show that a plural target is
\textbf{more likely} following a plural GEN-marked attractor (24\%)
compared to a singular one (11.4\%). In contrast, for NOM-marked
attractors, a plural target is \textbf{less likely} following a plural
attractor (5\%) than a singular one (17\%). If attraction were driven
purely by predictive probability (rational inference), we would expect
significantly larger attraction effects for the Genitive condition.

\subsection{Bayesian Model Results}\label{bayesian-model-results}

We fitted a Bayesian logistic regression model to quantify the effects.
We utilized a custom contrast coding scheme: 1. \textbf{Grammaticality}:
Target Match vs.~(Attractor Match + No Match) 2. \textbf{Attraction}:
Attractor Match vs.~No Match

The model summary (Figure 3) reveals a robust main effect of
\textbf{Attraction} (\(\hat{\beta}\) = 0.29, 95\% CrI {[}0.03, 0.54{]},
\(P(\beta > 0)\) = 0.98), confirming the presence of agreement
attraction. Crucially, however, the interaction between Case and
Attraction was not significant (\(\hat{\beta}\) = -0.05, 95\% CrI
{[}-0.32, 0.21{]}, \(P(\beta > 0)\) = 0.34). This indicates that the
magnitude of the attraction effect did not reliably differ between the
Genitive and Nominative conditions.

\begin{figure}[H]

\caption{Posterior probabilities of model coefficients.}

{\centering \pandocbounded{\includegraphics[keepaspectratio]{paper_files/figure-pdf/coef_plot-1.pdf}}

}

\end{figure}%

\section{Discussion}\label{discussion}

\subsection{Summary of Findings}\label{summary-of-findings}

In this study, we investigated the effect of attractor case on agreement
attraction in Turkish possessive constructions. We compared Genitive
attractors, which are structural validity predictors of upcoming
agreement, against Nominative (bare) attractors, which are the canonical
form of subjects. Our primary finding is that both attractor types
elicit comparable magnitudes of agreement attraction. Specifically, our
Bayesian analysis revealed a robust main effect of Attraction but no
significant interaction between Case and Attraction. This null result
persists despite the fact that Genitive attractors are much stronger
probabilistic predictors of plural targets in the corpus.

\subsection{Theoretical Implications}\label{theoretical-implications}

These results provide critical constraints for models of agreement
processing. The lack of modulation by case challenge pure
\textbf{probabilistic inference} accounts. If the parser were simply
tracking the transitional probability
\(P(\text{Plural Target} | \text{Attractor})\) to repair the string, we
would have expected significantly stronger attraction in the Genitive
condition, where the attractor strongly signals an upcoming plural head.
The uniform attraction effect suggests that the mechanism driving these
errors is insensitive to fine-grained co-occurrence statistics of this
type.

Instead, the results favor \textbf{structural controllerhood} accounts.
Both Genitive possessors and Nominative subjects are licensed agreement
controllers in Turkish grammar. The parser appears to retrieve
candidates based on this abstract structural status---searching for
``things that can control agreement''---rather than relying solely on
surface feature overlap (which would favor Nominative) or predictive
probability (which would favor Genitive). This supports a view of memory
retrieval that is structurally constrained, targeting items with
specific syntactic licensing properties.

\subsection{LLM and Attention
Mechanisms}\label{llm-and-attention-mechanisms}

Our findings also align with recent insights from computational models.
Transformer-based Large Language Models (LLMs), such as BERT, rely on
self-attention mechanisms to track dependencies. Previous work has shown
that these models often attend to attractors in a way that correlates
with human error patterns Linzen et al.
(\citeproc{ref-linzen2016assessing}{2016}); Arehalli and Linzen
(\citeproc{ref-arehalli2020neural}{2020}). For Turkish, the fact that
BERT and similar models attend to both Genitive and Nominative
attractors suggests that ``attention'' in these systems may be capturing
the same notion of ``structural controllerhood'' that humans use. The
model learns that both case forms are relevant for agreement
computation, and thus distributes attention to them, leading to similar
``errors'' or interference effects. This provides a computational
correlate to the psycholinguistic construct of controllerhood: high
attention weights reflect the structural centrality of these case forms
in the language's dependency graph.

\subsection{Semantic Constraints on
Attraction}\label{semantic-constraints-on-attraction}

Finally, the behavior of the Nominative attractors speaks to the debate
on the role of semantic integration in agreement. In English, compounds
like \emph{rat-eater} are standard, whereas \emph{rats-eater} is
typically disallowed, suggesting that plural features are not
semantically interpreted inside such tight modifiers. Similarly, in our
Turkish materials, interpreting the bare attractor as a subject would
yield a semantically anomalous meaning (e.g., interpreting ``The
refugee's lawyer'' as ``The refugee {[}who is the lawyer{]}''), akin to
the ``rat-eater'' constraint where the modifier does not serve as the
independent argument.

Some accounts, such as Patson and Husband
(\citeproc{ref-patson2016misinterpretation}{2016}), argue that
attraction involves a ``good-enough'' reanalysis or misinterpretation,
where the parser briefly entertains the ungrammatical dependency as a
valid, meaningful alternative (i.e., actually interpreting the attractor
as the subject). If this were the case, the gross semantic
implausibility of the Nominative attractor interpretation should block
attraction, or at least significantly reduce it compared to the
semantically more viable Genitive possessor.

However, our results show robust attraction for these semantically
``abnormal'' Nominative attractors. This aligns more closely with
retrieval-based accounts that posit attraction as a fast, automatic
error in the retrieval mechanism itself, rather than a full reanalysis
or misinterpretation Schlueter et al.
(\citeproc{ref-schlueter2019error}{2019}); Parker
(\citeproc{ref-parker2018memory}{2018}). The parser retrieves the
attractor because it matches the structural/morphological retrieval cues
(specifically, checking for a ``subject-like'' controller), but this
retrieval does not necessarily imply that the system has successfully
integrated the attractor into a coherent, meaningful interpretation of
the sentence. Thus, the persistence of attraction in the face of
semantic implausibility supports the view that the agreement mechanism
is encapsulated from fine-grained semantic plausibility checks during
the initial retrieval phase.

\section{Conclusion}\label{conclusion}

We found similar rates of agreement attraction with two differently
case-marked attractors in Turkish. Despite differences in their
predictive validity for plural targets, both Genitive and Nominative
attractors elicited comparable error rates. This supports a view of
sentence processing where structural properties---specifically, the
potential to control agreement---weigh heavier than surface
probabilistic cues in driving memory retrieval and agreement
computation.

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-arehalli2020neural}
Arehalli, S., \& Linzen, T. (2020). Neural networks as cognitive models
of agreement attraction. \emph{Proceedings of the Society for
Computation in Linguistics}, \emph{3}(1), 335--345.

\bibitem[\citeproctext]{ref-bhatia2011agreement}
Bhatia, A., \& Dillon, B. (2011). Agreement attraction in comprehension:
The role of feature match and structural controllerhood. \emph{Journal
of Experimental Psychology: Learning, Memory, and Cognition}.

\bibitem[\citeproctext]{ref-cartner2025number}
Cartner, M. et al. (2025). Number attraction in turkish genitives.
\emph{Proceedings of the 38th Annual Conference on Human Sentence
Processing}.

\bibitem[\citeproctext]{ref-linzen2016assessing}
Linzen, T., Dupoux, E., \& Goldberg, Y. (2016). Assessing the ability of
LSTMs to learn syntax-sensitive dependencies. \emph{Transactions of the
Association for Computational Linguistics}, \emph{4}, 521--535.

\bibitem[\citeproctext]{ref-parker2018memory}
Parker, D. (2018). Memory retrieval in sentence processing. \emph{Topics
in Cognitive Science}, \emph{10}(1), 117--133.

\bibitem[\citeproctext]{ref-patson2016misinterpretation}
Patson, N. D., \& Husband, E. M. (2016). Misinterpretation in agreement
attraction. \emph{Quarterly Journal of Experimental Psychology},
\emph{69}(5), 950--971.

\bibitem[\citeproctext]{ref-ryu2025agreement}
Ryu, S.-Y., \& Lewis, R. L. (2025). Agreement attraction in turkish: A
comparison of human and model behavior. \emph{Manuscript in
Preparation}.

\bibitem[\citeproctext]{ref-schlueter2019error}
Schlueter, Z., Parker, D., \& Lau, E. (2019). Error-driven retrieval in
agreement attraction. \emph{Language, Cognition and Neuroscience},
\emph{34}(8), 1042--1056.

\bibitem[\citeproctext]{ref-vasishth2008integration}
Vasishth, S., \& Lewis, R. L. (2008). Integration processes and
expectation in argument resolution. \emph{Journal of Memory and
Language}, \emph{59}(4), 439--457.

\bibitem[\citeproctext]{ref-wagers2009agreement}
Wagers, M. W., Lau, E. F., \& Phillips, C. (2009). Agreement attraction
in comprehension: Representations and processes. \emph{Journal of Memory
and Language}, \emph{61}(2), 206--237.

\end{CSLReferences}

\newpage

\appendix

\section{Appendices}\label{apx-appendices}

\subsection{Appendix A: Turkish Case System and Attractor
Types}\label{appendix-a-turkish-case-system-and-attractor-types}

Maybe some stuff can be offloaded here.

To understand the structural difference between the attractors used in
this study, it is necessary to distinguish between the
\textbf{Genitive-Possessive} construction and the \textbf{Nominative
(Bare)} construction in Turkish.

\textbf{1. Genitive Attractors (Possessors)} In Turkish, a definite
possessive construction involves a Genitive case marker on the possessor
and a Possessive marker on the head noun (the agreement target). *
\emph{Example:} \texttt{Mülteci-nin\ avukat-ı} (The refugee's lawyer) *
\emph{Structural Role:} The Genitive noun (\texttt{Mülteci-nin}) acts as
a specifier and is structurally licensed as a possessor. In Turkish,
possessors are strong controllers of agreement (e.g., in nominalized
clauses).

\textbf{2. Nominative Attractors (Modifiers)} The ``Nominative''
attractors in this study consist of bare nouns appearing in a modifier
position. While they carry no overt case morphology (Nominative), they
do not function as true possessors in this context. * \emph{Example:}
\texttt{Mülteci\ avukat-ı} (The refugee lawyer / The lawyer of refugees)
* \emph{Structural Role:} The bare noun (\texttt{Mülteci}) acts as a
modifier (similar to an English compound like \emph{Refugee lawyer}).
Unlike the Genitive possessor, it is not structurally poised to control
agreement, making it a crucial test case for ``Structural
Controllerhood'' theories.

\subsection{Appendix B: Corpus Analysis
Details}\label{appendix-b-corpus-analysis-details}

We utilized the \textbf{TrTenTen} corpus (approx. 4.9 billion words) to
calculate the transitional probabilities reported in the study. The goal
was to determine how predictive each attractor type is of an upcoming
plural target.

We calculated the conditional probability
\(P(\text{Plural Target} | \text{Attractor})\) for both Genitive and
Nominative attractors.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2162}}
  >{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2432}}
  >{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5405}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Attractor Case
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Attractor Number
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Probability of Following Plural Target
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Genitive} & Singular & 11.4\% \\
\textbf{Genitive} & Plural & \textbf{24.0\%} \\
\textbf{Nominative} & Singular & 17.0\% \\
\textbf{Nominative} & Plural & 5.0\% \\
\end{longtable}

\emph{Note.} Genitive plural attractors strongly predict a plural head,
creating a high-expectancy environment for a plural verb. Nominative
plural attractors do not share this predictive validity.

\subsection{Appendix C: Large Language Model
Analysis}\label{appendix-c-large-language-model-analysis}

To compare human processing with computational models, we analyzed
\textbf{Turkish BERT} (uncased). We focused on the model's
self-attention mechanism to quantify ``interference.''

\textbf{Methodology} 1. \textbf{Head Selection:} We first identified
attention heads specialized for the subject-verb dependency
(\texttt{nsubj}) using a standard probing task on a held-out dataset. 2.
\textbf{Attention Difference:} For every experimental item, we extracted
the attention weight from the Verb to the Attractor (\(w_{att}\)) and
from the Verb to the Target (\(w_{tgt}\)). 3. \textbf{Scoring:} We
calculated the difference (\(w_{att} - w_{tgt}\)). A positive score
indicates the model is ``distracted'' by the attractor.

\textbf{Results} As shown in the main text (Figure 1), the model
exhibited significant interference (positive attention difference) for
both Genitive and Nominative attractors, mirroring the human behavioral
results where structural status outweighed probabilistic cues.






\end{document}
